{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e1bd89-a9b0-491d-aba3-2f5862fa1bdc",
   "metadata": {},
   "source": [
    "XGBoost (Extreme Gradient Boosting) is a powerful and popular machine learning algorithm known for its speed and performance in predictive modeling tasks. It is an implementation of gradient boosting that optimizes both the speed and accuracy of the model. Here’s how it works:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Boosting Concept**\n",
    "XGBoost is based on the concept of **boosting**, which is an ensemble technique. In boosting, multiple weak learners (typically decision trees) are combined to create a strong predictive model. Each tree tries to correct the errors made by the previous one.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Gradient Boosting Mechanism**\n",
    "- **Step 1:** Start with an initial prediction. This is usually the mean value for regression or uniform probability for classification.\n",
    "- **Step 2:** Calculate the residuals (errors) between the predicted values and the actual values.\n",
    "- **Step 3:** Train a new decision tree to predict these residuals.\n",
    "- **Step 4:** Add the predictions of this new tree to the previous predictions, adjusting with a learning rate (η).\n",
    "- **Step 5:** Repeat steps 2-4 for a predefined number of trees or until the error is minimized.\n",
    "\n",
    "The model is updated as:\n",
    "$$\n",
    "F_{m}(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "$$\n",
    "where:\n",
    "- \\( F_m(x) \\) = Prediction from the m-th tree\n",
    "- \\( h_m(x) \\) = Residuals predicted by the m-th tree\n",
    "- \\( \\eta \\) = Learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Objective Function**\n",
    "XGBoost optimizes an objective function that consists of two parts:\n",
    "$$\n",
    "Obj = L(\\theta) + \\Omega(\\theta)\n",
    "$$\n",
    "- \\( L(\\theta) \\): Loss function (e.g., Mean Squared Error for regression, Log Loss for classification) to measure model performance.\n",
    "- \\( \\Omega(\\theta) \\): Regularization term to control model complexity, which helps to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Regularization and Pruning**\n",
    "- XGBoost uses **L1 (LASSO) and L2 (Ridge)** regularization to penalize large coefficients, improving model generalization.\n",
    "- **Tree Pruning:** It uses a technique called **\"max depth\"** to control the depth of trees, preventing them from growing too complex.\n",
    "- It also employs **\"maximum delta step\"** to ensure that updates are within a reasonable range, stabilizing the learning process.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Handling Missing Values**\n",
    "XGBoost is capable of handling missing values by learning the best direction to take when it encounters a missing value in the data during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Parallelization and Efficiency**\n",
    "- XGBoost is highly efficient due to its parallelized tree construction.\n",
    "- It uses **Histogram-based split finding**, which reduces the computation cost by grouping continuous features into discrete bins.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Advantages of XGBoost**\n",
    "- High predictive accuracy\n",
    "- Fast training speed due to parallel computation\n",
    "- Robustness to overfitting with built-in regularization\n",
    "- Handles missing values naturally\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Applications**\n",
    "XGBoost is widely used in various applications, including:\n",
    "- Classification (e.g., Churn Prediction)\n",
    "- Regression (e.g., Predicting House Prices)\n",
    "- Ranking (e.g., Search Engine Ranking)\n",
    "- Clustering and Feature Engineering\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15039de2-d3ed-4a7f-9d1f-be58806d25b3",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install xgboost scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d91cc1-7c55-42dc-bd39-629c16abe6f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tbaka\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\tbaka\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\tbaka\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tbaka\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tbaka\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/124.9 MB 5.6 MB/s eta 0:00:23\n",
      "    --------------------------------------- 2.9/124.9 MB 7.3 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 4.5/124.9 MB 7.5 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 5.2/124.9 MB 6.9 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 6.6/124.9 MB 6.4 MB/s eta 0:00:19\n",
      "   -- ------------------------------------- 8.4/124.9 MB 6.8 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 9.7/124.9 MB 6.7 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 12.1/124.9 MB 7.3 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 13.9/124.9 MB 7.5 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 16.3/124.9 MB 7.8 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 18.4/124.9 MB 8.0 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 20.4/124.9 MB 8.1 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 22.3/124.9 MB 8.2 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 24.6/124.9 MB 8.4 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 27.0/124.9 MB 8.6 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 28.6/124.9 MB 8.6 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 30.7/124.9 MB 8.6 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 32.8/124.9 MB 8.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 34.6/124.9 MB 8.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 36.4/124.9 MB 8.7 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 38.5/124.9 MB 8.8 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 40.4/124.9 MB 8.7 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 42.5/124.9 MB 8.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 44.0/124.9 MB 8.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 45.6/124.9 MB 8.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 47.2/124.9 MB 8.6 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 48.5/124.9 MB 8.6 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 50.6/124.9 MB 8.6 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 53.0/124.9 MB 8.7 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 55.1/124.9 MB 8.7 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 56.6/124.9 MB 8.7 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 58.5/124.9 MB 8.7 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 60.6/124.9 MB 8.7 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 62.9/124.9 MB 8.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 65.3/124.9 MB 8.8 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 67.4/124.9 MB 8.9 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 69.5/124.9 MB 8.9 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 71.3/124.9 MB 8.9 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 73.1/124.9 MB 8.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 75.0/124.9 MB 8.9 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 77.3/124.9 MB 8.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 79.7/124.9 MB 9.0 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 82.1/124.9 MB 9.0 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 84.4/124.9 MB 9.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 87.0/124.9 MB 9.1 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 89.7/124.9 MB 9.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 91.8/124.9 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 93.8/124.9 MB 9.2 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 96.2/124.9 MB 9.3 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 98.0/124.9 MB 9.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 100.4/124.9 MB 9.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 102.5/124.9 MB 9.3 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 105.1/124.9 MB 9.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 107.2/124.9 MB 9.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 109.6/124.9 MB 9.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 111.7/124.9 MB 9.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 114.0/124.9 MB 9.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 116.4/124.9 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 118.8/124.9 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 121.1/124.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  123.5/124.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  124.8/124.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 124.9/124.9 MB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df20d16-5974-43b5-bf83-1bd44f2e31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      1.00      1.00         9\n",
      "   virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbaka\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:58:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Loading the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Setting the hyperparameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Multiclass classification\n",
    "    'num_class': 3,               # Number of classes\n",
    "    'learning_rate': 0.1,         # Step size shrinkage\n",
    "    'max_depth': 3,               # Maximum depth of trees\n",
    "    'n_estimators': 100,          # Number of boosting rounds\n",
    "    'seed': 42                    # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Training the model\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Making predictions\n",
    "preds = model.predict(dtest)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, preds, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6be2b3-4f90-458f-a263-6be0e9986148",
   "metadata": {},
   "source": [
    "## 3. **Explanation of Key Hyperparameters**\n",
    "\n",
    "1. **`objective`:**  \n",
    "   - `multi:softmax`: Used for multi-class classification.\n",
    "   - `multi:softprob`: Returns probabilities of each class.\n",
    "   - `binary:logistic`: For binary classification.\n",
    "\n",
    "2. **`num_class`:**  \n",
    "   - Number of classes (required for multiclass classification).\n",
    "\n",
    "3. **`learning_rate` (alias: `eta`):**  \n",
    "   - Controls the step size at each boosting iteration.\n",
    "   - Lower values make the model more robust to overfitting but require more boosting rounds.\n",
    "\n",
    "4. **`max_depth`:**  \n",
    "   - Maximum depth of a tree.\n",
    "   - Higher values make the model more complex and likely to overfit.\n",
    "\n",
    "5. **`n_estimators`:**  \n",
    "   - Number of boosting rounds or trees.\n",
    "\n",
    "6. **`seed`:**  \n",
    "   - Random seed for reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Hyperparameter Tuning**\n",
    "\n",
    "To improve model performance, you can tune the hyperparameters using **Grid Search** or **Randomized Search**. Here's an example using `RandomizedSearchCV`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a7543f-88f5-45e9-8497-8e0071f99f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a04608-0627-4c41-b381-9476a397a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.8; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, gamma=0, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, gamma=0, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, gamma=0, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.5s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.05, max_depth=3, n_estimators=50, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.05, max_depth=3, n_estimators=50, subsample=0.6; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.05, max_depth=3, n_estimators=50, subsample=0.6; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.05, max_depth=7, n_estimators=50, subsample=0.6; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.05, max_depth=7, n_estimators=50, subsample=0.6; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.05, max_depth=7, n_estimators=50, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.2s\n",
      "Best Parameters: {'subsample': 0.8, 'n_estimators': 50, 'max_depth': 7, 'learning_rate': 0.01, 'gamma': 0.3, 'colsample_bytree': 1.0}\n",
      "Best Accuracy: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Defining the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# Creating the model\n",
    "xgb_model = XGBClassifier(objective='multi:softmax', num_class=3, use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Performing Randomized Search\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_grid, n_iter=10, cv=3, scoring='accuracy', verbose=2, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Accuracy:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03f236-99be-404a-ad9d-c246047ee1e2",
   "metadata": {},
   "source": [
    "## 5. **Why Use RandomizedSearchCV?**\n",
    "- It searches a random subset of the hyperparameter space, making it faster than Grid Search.\n",
    "- It often finds good hyperparameter values with fewer iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Next Steps**\n",
    "- You can try this with other datasets like **Churn Prediction** or **Facial Emotion Recognition**.\n",
    "- If you want, we can also explore **Cross-Validation** techniques in XGBoost for more robust evaluation.\n",
    "\n",
    "Would you like help with more advanced topics like feature importance in XGBoost or deploying an XGBoost model with FastAPI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1af24-718c-44fc-825f-1f3637d7237e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
